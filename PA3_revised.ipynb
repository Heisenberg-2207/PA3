{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "num_actions = env.action_space\n",
    "num_obs = env.observation_space\n",
    "R, G, B, Y = env.unwrapped.locs\n",
    "R, G, B, Y = list(R), list(G), list(B), list(Y)\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n",
      "[0, 0] [0, 4] [4, 0] [4, 3]\n"
     ]
    }
   ],
   "source": [
    "def get_state(state):\n",
    "    row, col, pass_id, dest_id = env.unwrapped.decode(state)\n",
    "    state = np.asarray([row,col,pass_id,dest_id])\n",
    "    return state\n",
    "\n",
    "print(get_state(123)[0:2])\n",
    "print(R,G,B,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi at [1 1]\n"
     ]
    }
   ],
   "source": [
    "curr_state = get_state(env.s)\n",
    "print(\"Taxi at\",curr_state[0:2])\n",
    "next_state, reward, done, _, _ = env.step(4)\n",
    "row, col, pass_id, dest_id = env.unwrapped.decode(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_r = np.zeros((500,4))\n",
    "q_b = np.zeros((500,4))\n",
    "q_g = np.zeros((500,4))\n",
    "q_y = np.zeros((500,4))\n",
    "q = np.zeros((500,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of episodes and maximum number of steps per episode\n",
    "EPS = 10000\n",
    "MAX_STEPS = 500\n",
    "EXP = 1\n",
    "# Set the learning rate, discount factor, and exploration rate\n",
    "ALPHA = 0.15\n",
    "GAMMA = 0.99\n",
    "exploration_rate = 1.0\n",
    "MIN_EXP = 0.01\n",
    "EXP_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state,qs,eps):\n",
    "    #print(np.shape(q))\n",
    "    length = np.shape(qs)[1]\n",
    "    if not qs[state].any():\n",
    "        return random.randint(0,length-1)\n",
    "    action = np.argmax(qs[state])\n",
    "    \n",
    "    if np.random.rand() < eps:\n",
    "        action = np.random.randint(0,length-1)\n",
    "        return action\n",
    "    return action   \n",
    "\n",
    "def choose_action_option(state,qs):\n",
    "    action = np.argmax(qs[state])\n",
    "    return action  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options where policy is greedy wrt the corresponding q Value function\n",
    "def Red(q_r,state,eps):\n",
    "    optdone = False\n",
    "    optact = choose_action(state,q_r,eps)\n",
    "    state = get_state(state)\n",
    "    if state[0] == R[0] and state[1] == R[1]:\n",
    "        optdone = True\n",
    "    return optact,optdone\n",
    "\n",
    "def Green(q_g,state,eps):\n",
    "    optdone = False\n",
    "    optact = choose_action(state,q_g,eps)\n",
    "    state = get_state(state)\n",
    "    if state[0] == G[0] and state[1] == G[1]:\n",
    "        optdone = True\n",
    "    return optact,optdone\n",
    "    \n",
    "def Yellow(q_y,state,eps):\n",
    "    optdone = False\n",
    "    optact = choose_action(state,q_y,eps)\n",
    "    state = get_state(state)\n",
    "    if state[0] == Y[0] and state[1] == Y[1]:\n",
    "        optdone = True\n",
    "    return optact,optdone \n",
    "\n",
    "def Blue(q_b,state,eps):\n",
    "    optdone = False\n",
    "    optact = choose_action(state,q_b,eps)\n",
    "    state = get_state(state)\n",
    "    if state[0] == B[0] and state[1] == B[1]:\n",
    "        optdone = True\n",
    "    return optact,optdone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMDP:\n",
    "    def __init__(self):\n",
    "        self.avg_reward = []\n",
    "        self.total_reward = []\n",
    "\n",
    "    def Qlearn(self):\n",
    "        epsilon = 0.1\n",
    "        for episode in range(5000):\n",
    "            running = 0\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            ep_reward = 0\n",
    "            steps = 0\n",
    "            while not done :\n",
    "                steps += 1\n",
    "                action = choose_action(state,q,epsilon)\n",
    "                if action < 6:\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    ep_reward+= reward\n",
    "                    q[state,action] = q[state, action] + ALPHA * (reward + GAMMA * np.max(q[next_state, :])- q[state, action])\n",
    "                    state = next_state\n",
    "                reward_bar = 0\n",
    "                if action > 5 :\n",
    "                    count = 0\n",
    "                    optdone = False\n",
    "                    current_state = state\n",
    "                    while (optdone == False) :\n",
    "                        if action == 6:\n",
    "                            optact, optdone = Red(q_r,state,epsilon) \n",
    "                            next_state, reward, done, _, _ = env.step(optact)\n",
    "                            q_r[state,optact] = q_r[state, optact] + ALPHA * (reward + GAMMA * np.max(q_r[next_state, :])- q_r[state, optact])\n",
    "                        if action == 7:\n",
    "                            optact, optdone = Green(q_g,state,epsilon)\n",
    "                            next_state, reward, done, _, _ = env.step(optact)\n",
    "                            q_g[state,optact] = q_g[state, optact] + ALPHA * (reward + GAMMA * np.max(q_g[next_state, :])- q_g[state, optact])\n",
    "                        if action == 8:\n",
    "                            optact, optdone = Blue(q_b,state,epsilon)\n",
    "                            next_state, reward, done, _, _ = env.step(optact)\n",
    "                            q_b[state,optact] = q_b[state, optact] + ALPHA * (reward + GAMMA * np.max(q_b[next_state, :])- q_b[state, optact])\n",
    "                        if action == 9:\n",
    "                            optact, optdone = Yellow(q_y,state,epsilon)\n",
    "                            next_state, reward, done, _, _ = env.step(optact)\n",
    "                            q_y[state,optact] = q_y[state, optact] + ALPHA * (reward + GAMMA * np.max(q_y[next_state])- q_y[state, optact])\n",
    "                        reward_bar =  reward_bar + (GAMMA**count)*reward\n",
    "                        ep_reward += reward\n",
    "                        count += 1\n",
    "                        if optdone == True:\n",
    "                            q[current_state, action] += ALPHA * (reward_bar - q[current_state, action] + (GAMMA**count) * np.max(q[next_state, :]))\n",
    "                        state = next_state\n",
    "                if epsilon > 0.01:\n",
    "                    epsilon = epsilon*np.exp(-0.01)\n",
    "            running = 0.35 * ep_reward + (1 - 0.35) * running\n",
    "            self.avg_reward.append(running)\n",
    "            self.q=q\n",
    "            self.total_reward.append(ep_reward)\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {ep_reward}\")\n",
    "        return self.avg_reward, self.total_reward\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.plot(self.avg_reward,label=\"Moving Average\")\n",
    "        plt.grid()\n",
    "        plt.plot(self.total_reward,label=\"Total Rewards\",alpha=1, color='orange')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Averaged Episodic Returns')\n",
    "        plt.title(\"Averaged Episodic Returns\")\n",
    "        plt.legend()\n",
    "        plt.savefig('SMDP.png')\n",
    "        plt.show()\n",
    "\n",
    "    def render_run(self):\n",
    "        special_env = gym.make('Taxi-v3',render_mode='human')\n",
    "        done = False\n",
    "        eps = 0.01\n",
    "        state, _ = special_env.reset()\n",
    "        while not done:\n",
    "            action = choose_action(state,self.q,eps)\n",
    "            next_state, reward, done, _, _ = special_env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    def plot_Q(q, q_f):\n",
    "        ACTIONS = {0: \"South\",1: \"Up\",2: \"East\",3: \"West\",4: \"Pick\",5: \"Drop\",6: \"Red\",7: \"Green\",8: \"Blue\", 9: \"Yellow\"}\n",
    "    # decode each state \n",
    "    # ((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination\n",
    "        action = np.argmax(q,axis=1)\n",
    "        action_f = np.argmax(q_f,axis=1)\n",
    "\n",
    "        dest_r = action[::4]\n",
    "        dest_g = action[1::4]\n",
    "        dest_b = action[2::4]\n",
    "        dest_y = action[3::4]\n",
    "\n",
    "        dest_r_f = action_f[::4]\n",
    "        dest_g_f = action_f[1::4]\n",
    "        dest_b_f = action_f[2::4]\n",
    "        dest_y_f = action_f[3::4]\n",
    "\n",
    "        loc_r_r = dest_r[::5]\n",
    "        loc_r_g = dest_g[::5]\n",
    "        loc_r_b = dest_b[::5]\n",
    "        loc_r_y = dest_y[::5]\n",
    "\n",
    "        loc_r_r_f = dest_r_f[::5]\n",
    "        loc_r_g_f = dest_g_f[::5]\n",
    "        loc_r_b_f = dest_b_f[::5]\n",
    "        loc_r_y_f = dest_y_f[::5]\n",
    "\n",
    "        loc_g_r = dest_r[1::5]\n",
    "        loc_g_g = dest_g[1::5]\n",
    "        loc_g_b = dest_b[1::5]\n",
    "        loc_g_y = dest_y[1::5]\n",
    "\n",
    "        loc_g_r_f = dest_r_f[1::5]\n",
    "        loc_g_g_f = dest_g_f[1::5]\n",
    "        loc_g_b_f = dest_b_f[1::5]\n",
    "        loc_g_y_f = dest_y_f[1::5]\n",
    "\n",
    "        loc_b_r = dest_r[2::5]\n",
    "        loc_b_g = dest_g[2::5]\n",
    "        loc_b_b = dest_b[2::5]\n",
    "        loc_b_y = dest_y[2::5]\n",
    "\n",
    "        loc_b_r_f = dest_r_f[2::5]\n",
    "        loc_b_g_f = dest_g_f[2::5]\n",
    "        loc_b_b_f = dest_b_f[2::5]\n",
    "        loc_b_y_f = dest_y_f[2::5]\n",
    "\n",
    "        loc_y_r = dest_r[3::5]\n",
    "        loc_y_g = dest_g[3::5]\n",
    "        loc_y_b = dest_b[3::5]\n",
    "        loc_y_y = dest_y[3::5]\n",
    "\n",
    "        loc_y_r_f = dest_r_f[3::5]\n",
    "        loc_y_g_f = dest_g_f[3::5]\n",
    "        loc_y_b_f = dest_b_f[3::5]\n",
    "        loc_y_y_f = dest_y_f[3::5]\n",
    "\n",
    "    # Create a 5x5 grid for grid and frequncy\n",
    "        grid = np.reshape(loc_r_g, (5, 5))\n",
    "        freq = np.reshape(loc_r_g_f, (5, 5))\n",
    "\n",
    "    # Display the grid with numbers\n",
    "        plt.imshow(freq, cmap='viridis', interpolation='nearest')\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                text = ACTIONS[grid[i, j]]\n",
    "                plt.text(j, i, text, ha='center', va='center', color='black')\n",
    "\n",
    "    # Add labels and title\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('Column')\n",
    "        plt.ylabel('Row')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -5394\n",
      "Episode 2: Total Reward = -4927\n",
      "Episode 3: Total Reward = -5866\n",
      "Episode 4: Total Reward = -4467\n",
      "Episode 5: Total Reward = -6196\n",
      "Episode 6: Total Reward = -1090\n",
      "Episode 7: Total Reward = -5255\n",
      "Episode 8: Total Reward = -1739\n",
      "Episode 9: Total Reward = -3169\n",
      "Episode 10: Total Reward = -6228\n",
      "Episode 11: Total Reward = -3490\n",
      "Episode 12: Total Reward = -1380\n",
      "Episode 13: Total Reward = -3452\n",
      "Episode 14: Total Reward = -786\n",
      "Episode 15: Total Reward = -1621\n",
      "Episode 16: Total Reward = -1188\n",
      "Episode 17: Total Reward = -4755\n",
      "Episode 18: Total Reward = -4771\n",
      "Episode 19: Total Reward = -1208\n",
      "Episode 20: Total Reward = -1290\n",
      "Episode 21: Total Reward = -230\n",
      "Episode 22: Total Reward = -1291\n",
      "Episode 23: Total Reward = -1488\n",
      "Episode 24: Total Reward = -6252\n",
      "Episode 25: Total Reward = -1014\n",
      "Episode 26: Total Reward = -2602\n",
      "Episode 27: Total Reward = -2423\n",
      "Episode 28: Total Reward = -1469\n",
      "Episode 29: Total Reward = -556\n",
      "Episode 30: Total Reward = -566\n",
      "Episode 31: Total Reward = -1969\n",
      "Episode 32: Total Reward = -1722\n",
      "Episode 33: Total Reward = -1047\n",
      "Episode 34: Total Reward = -953\n",
      "Episode 35: Total Reward = -1420\n",
      "Episode 36: Total Reward = -1168\n",
      "Episode 37: Total Reward = -957\n",
      "Episode 38: Total Reward = -1355\n",
      "Episode 39: Total Reward = -1110\n",
      "Episode 40: Total Reward = -806\n",
      "Episode 41: Total Reward = -564\n",
      "Episode 42: Total Reward = -123\n",
      "Episode 43: Total Reward = -538\n",
      "Episode 44: Total Reward = -1230\n",
      "Episode 45: Total Reward = -831\n",
      "Episode 46: Total Reward = -326\n",
      "Episode 47: Total Reward = -1695\n",
      "Episode 48: Total Reward = -713\n",
      "Episode 49: Total Reward = -386\n",
      "Episode 50: Total Reward = -919\n",
      "Episode 51: Total Reward = -821\n",
      "Episode 52: Total Reward = -427\n",
      "Episode 53: Total Reward = -340\n",
      "Episode 54: Total Reward = -1265\n",
      "Episode 55: Total Reward = -941\n",
      "Episode 56: Total Reward = -1561\n",
      "Episode 57: Total Reward = -722\n",
      "Episode 58: Total Reward = -393\n",
      "Episode 59: Total Reward = -2570\n",
      "Episode 60: Total Reward = -551\n",
      "Episode 61: Total Reward = -560\n",
      "Episode 62: Total Reward = -217\n",
      "Episode 63: Total Reward = -763\n",
      "Episode 64: Total Reward = -384\n",
      "Episode 65: Total Reward = -476\n",
      "Episode 66: Total Reward = -1800\n",
      "Episode 67: Total Reward = -991\n",
      "Episode 68: Total Reward = -823\n",
      "Episode 69: Total Reward = -644\n",
      "Episode 70: Total Reward = -3309\n",
      "Episode 71: Total Reward = -1744\n",
      "Episode 72: Total Reward = -1407\n",
      "Episode 73: Total Reward = -469\n",
      "Episode 74: Total Reward = -778\n",
      "Episode 75: Total Reward = -2388\n",
      "Episode 76: Total Reward = -751\n",
      "Episode 77: Total Reward = -458\n",
      "Episode 78: Total Reward = -11\n",
      "Episode 79: Total Reward = -142\n",
      "Episode 80: Total Reward = -928\n",
      "Episode 81: Total Reward = -680\n",
      "Episode 82: Total Reward = -306\n",
      "Episode 83: Total Reward = -240\n",
      "Episode 84: Total Reward = -676\n",
      "Episode 85: Total Reward = -1084\n",
      "Episode 86: Total Reward = -480\n",
      "Episode 87: Total Reward = -381\n",
      "Episode 88: Total Reward = -116\n",
      "Episode 89: Total Reward = -190\n",
      "Episode 90: Total Reward = -273\n",
      "Episode 91: Total Reward = -157\n",
      "Episode 92: Total Reward = 5\n",
      "Episode 93: Total Reward = -582\n",
      "Episode 94: Total Reward = -178\n",
      "Episode 95: Total Reward = -246\n",
      "Episode 96: Total Reward = -693\n",
      "Episode 97: Total Reward = -1213\n",
      "Episode 98: Total Reward = -1652\n",
      "Episode 99: Total Reward = -447\n",
      "Episode 100: Total Reward = -805\n",
      "Episode 101: Total Reward = -353\n",
      "Episode 102: Total Reward = -498\n",
      "Episode 103: Total Reward = -540\n",
      "Episode 104: Total Reward = -774\n",
      "Episode 105: Total Reward = -370\n",
      "Episode 106: Total Reward = -615\n",
      "Episode 107: Total Reward = 1\n",
      "Episode 108: Total Reward = -1321\n",
      "Episode 109: Total Reward = -247\n",
      "Episode 110: Total Reward = -319\n",
      "Episode 111: Total Reward = -1327\n",
      "Episode 112: Total Reward = -489\n",
      "Episode 113: Total Reward = -537\n",
      "Episode 114: Total Reward = -625\n",
      "Episode 115: Total Reward = -800\n",
      "Episode 116: Total Reward = -70\n",
      "Episode 117: Total Reward = -350\n",
      "Episode 118: Total Reward = -159\n",
      "Episode 119: Total Reward = -73\n",
      "Episode 120: Total Reward = -362\n",
      "Episode 121: Total Reward = -145\n",
      "Episode 122: Total Reward = -748\n",
      "Episode 123: Total Reward = -349\n",
      "Episode 124: Total Reward = -198\n",
      "Episode 125: Total Reward = -190\n",
      "Episode 126: Total Reward = -182\n",
      "Episode 127: Total Reward = -354\n",
      "Episode 128: Total Reward = -162\n",
      "Episode 129: Total Reward = -208\n",
      "Episode 130: Total Reward = -381\n",
      "Episode 131: Total Reward = -124\n",
      "Episode 132: Total Reward = -815\n",
      "Episode 133: Total Reward = -254\n",
      "Episode 134: Total Reward = -239\n",
      "Episode 135: Total Reward = -195\n",
      "Episode 136: Total Reward = -264\n",
      "Episode 137: Total Reward = -465\n",
      "Episode 138: Total Reward = -159\n",
      "Episode 139: Total Reward = -155\n",
      "Episode 140: Total Reward = -184\n",
      "Episode 141: Total Reward = -214\n",
      "Episode 142: Total Reward = -566\n",
      "Episode 143: Total Reward = -629\n",
      "Episode 144: Total Reward = -285\n",
      "Episode 145: Total Reward = -78\n",
      "Episode 146: Total Reward = -198\n",
      "Episode 147: Total Reward = -425\n",
      "Episode 148: Total Reward = -212\n",
      "Episode 149: Total Reward = -65\n",
      "Episode 150: Total Reward = -102\n",
      "Episode 151: Total Reward = -413\n",
      "Episode 152: Total Reward = -219\n",
      "Episode 153: Total Reward = 9\n",
      "Episode 154: Total Reward = -128\n",
      "Episode 155: Total Reward = -159\n",
      "Episode 156: Total Reward = -23\n",
      "Episode 157: Total Reward = -5\n",
      "Episode 158: Total Reward = -73\n",
      "Episode 159: Total Reward = -59\n",
      "Episode 160: Total Reward = -198\n",
      "Episode 161: Total Reward = -54\n",
      "Episode 162: Total Reward = -443\n",
      "Episode 163: Total Reward = -595\n",
      "Episode 164: Total Reward = -287\n",
      "Episode 165: Total Reward = -235\n",
      "Episode 166: Total Reward = -109\n",
      "Episode 167: Total Reward = -285\n",
      "Episode 168: Total Reward = -476\n",
      "Episode 169: Total Reward = -74\n",
      "Episode 170: Total Reward = -332\n",
      "Episode 171: Total Reward = -203\n",
      "Episode 172: Total Reward = -24\n",
      "Episode 173: Total Reward = -398\n",
      "Episode 174: Total Reward = -55\n",
      "Episode 175: Total Reward = -628\n",
      "Episode 176: Total Reward = -338\n",
      "Episode 177: Total Reward = -98\n",
      "Episode 178: Total Reward = -198\n",
      "Episode 179: Total Reward = -543\n",
      "Episode 180: Total Reward = -408\n",
      "Episode 181: Total Reward = -223\n",
      "Episode 182: Total Reward = -253\n",
      "Episode 183: Total Reward = 12\n",
      "Episode 184: Total Reward = -135\n",
      "Episode 185: Total Reward = -237\n",
      "Episode 186: Total Reward = -200\n",
      "Episode 187: Total Reward = -51\n",
      "Episode 188: Total Reward = -52\n",
      "Episode 189: Total Reward = -390\n",
      "Episode 190: Total Reward = -47\n",
      "Episode 191: Total Reward = -356\n",
      "Episode 192: Total Reward = -156\n",
      "Episode 193: Total Reward = -232\n",
      "Episode 194: Total Reward = -185\n",
      "Episode 195: Total Reward = -222\n",
      "Episode 196: Total Reward = -387\n",
      "Episode 197: Total Reward = -393\n",
      "Episode 198: Total Reward = -183\n",
      "Episode 199: Total Reward = -74\n",
      "Episode 200: Total Reward = 7\n",
      "Episode 201: Total Reward = -7\n",
      "Episode 202: Total Reward = -123\n",
      "Episode 203: Total Reward = -170\n",
      "Episode 204: Total Reward = -88\n",
      "Episode 205: Total Reward = 13\n",
      "Episode 206: Total Reward = -50\n",
      "Episode 207: Total Reward = -63\n",
      "Episode 208: Total Reward = -168\n",
      "Episode 209: Total Reward = -85\n",
      "Episode 210: Total Reward = -273\n",
      "Episode 211: Total Reward = -124\n",
      "Episode 212: Total Reward = -357\n",
      "Episode 213: Total Reward = -152\n",
      "Episode 214: Total Reward = -77\n",
      "Episode 215: Total Reward = -201\n",
      "Episode 216: Total Reward = -200\n",
      "Episode 217: Total Reward = -170\n",
      "Episode 218: Total Reward = -42\n",
      "Episode 219: Total Reward = -213\n",
      "Episode 220: Total Reward = -230\n",
      "Episode 221: Total Reward = -147\n",
      "Episode 222: Total Reward = -105\n",
      "Episode 223: Total Reward = -28\n",
      "Episode 224: Total Reward = -74\n",
      "Episode 225: Total Reward = 13\n",
      "Episode 226: Total Reward = -79\n",
      "Episode 227: Total Reward = -81\n",
      "Episode 228: Total Reward = -67\n",
      "Episode 229: Total Reward = -101\n",
      "Episode 230: Total Reward = -419\n",
      "Episode 231: Total Reward = -127\n",
      "Episode 232: Total Reward = -210\n",
      "Episode 233: Total Reward = -58\n",
      "Episode 234: Total Reward = -14\n",
      "Episode 235: Total Reward = -42\n",
      "Episode 236: Total Reward = 7\n",
      "Episode 237: Total Reward = -294\n",
      "Episode 238: Total Reward = -23\n",
      "Episode 239: Total Reward = 6\n",
      "Episode 240: Total Reward = 0\n",
      "Episode 241: Total Reward = -31\n",
      "Episode 242: Total Reward = -226\n",
      "Episode 243: Total Reward = -3\n",
      "Episode 244: Total Reward = -9\n",
      "Episode 245: Total Reward = -35\n",
      "Episode 246: Total Reward = -88\n",
      "Episode 247: Total Reward = -223\n",
      "Episode 248: Total Reward = -187\n",
      "Episode 249: Total Reward = -165\n",
      "Episode 250: Total Reward = -40\n",
      "Episode 251: Total Reward = 15\n",
      "Episode 252: Total Reward = -442\n",
      "Episode 253: Total Reward = -148\n",
      "Episode 254: Total Reward = -3\n",
      "Episode 255: Total Reward = 4\n",
      "Episode 256: Total Reward = -57\n",
      "Episode 257: Total Reward = -135\n",
      "Episode 258: Total Reward = -272\n",
      "Episode 259: Total Reward = -196\n",
      "Episode 260: Total Reward = -153\n",
      "Episode 261: Total Reward = -413"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m smdp \u001b[38;5;241m=\u001b[39m SMDP()\n\u001b[0;32m----> 2\u001b[0m \u001b[43msmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m smdp\u001b[38;5;241m.\u001b[39mplot()\n",
      "Cell \u001b[0;32mIn[20], line 56\u001b[0m, in \u001b[0;36mSMDP.Qlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq\u001b[38;5;241m=\u001b[39mq\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_reward\u001b[38;5;241m.\u001b[39mappend(ep_reward)\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpisode \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepisode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m: Total Reward = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mep_reward\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_reward\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/iostream.py:684\u001b[0m, in \u001b[0;36mOutStream.write\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;66;03m# only touch the buffer in the IO thread to avoid races\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_lock:\n\u001b[0;32m--> 684\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mfrozenset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_child:\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;66;03m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;66;03m# and this helps.\u001b[39;00m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subprocess_flush_pending:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "smdp = SMDP()\n",
    "smdp.Qlearn()\n",
    "smdp.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SMDP.plot_Q() missing 1 required positional argument: 'q_f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_Q\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: SMDP.plot_Q() missing 1 required positional argument: 'q_f'"
     ]
    }
   ],
   "source": [
    "smdp.plot_Q()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRA OPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_intra = np.zeros((500,10))\n",
    "\n",
    "class INTRAOPT:\n",
    "    def __init__(self):\n",
    "        self.avg_reward = []\n",
    "        self.total_reward = []\n",
    "\n",
    "    def Qlearn(self):\n",
    "        epsilon = 0.1\n",
    "        for episode in range(5000):\n",
    "            running = 0\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            ep_reward = 0\n",
    "            steps = 0\n",
    "            while not done :\n",
    "                steps += 1\n",
    "                action = choose_action(state,q_intra,epsilon)\n",
    "                if action < 6:\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    ep_reward+= reward\n",
    "                    q_intra[state,action] += ALPHA * (reward + GAMMA * np.max(q_intra[next_state, :])- q_intra[state, action])\n",
    "                    state = next_state\n",
    "                \n",
    "                if action > 5 :\n",
    "                    count = 0\n",
    "                    optdone = False\n",
    "                    while (optdone == False) :\n",
    "                        if action == 6:\n",
    "                            optact, optdone = Red(q_r,state,epsilon) \n",
    "                            next_state, reward, done, _, _ = env.step(optact)\n",
    "                            #q_r[state,optact] = q_r[state, optact] + ALPHA * (reward + GAMMA * np.max(q_r[next_state, :])- q_r[state, optact])\n",
    "                        if action == 7:\n",
    "                            optact, optdone = Green(q_g,state,epsilon)\n",
    "                            next_state, reward, done, _, _ = env.step(optact)\n",
    "                            #q_g[state,optact] = q_g[state, optact] + ALPHA * (reward + GAMMA * np.max(q_g[next_state, :])- q_g[state, optact])\n",
    "                        if action == 8:\n",
    "                            optact, optdone = Blue(q_b,state,epsilon)\n",
    "                            next_state, reward, done, _, _ = env.step(optact)\n",
    "                            #q_b[state,optact] = q_b[state, optact] + ALPHA * (reward + GAMMA * np.max(q_b[next_state, :])- q_b[state, optact])\n",
    "                        if action == 9:\n",
    "                            optact, optdone = Yellow(q_y,state,epsilon)\n",
    "                            next_state, reward, done, _, _ = env.step(optact)\n",
    "                            #q_y[state,optact] = q_y[state, optact] + ALPHA * (reward + GAMMA * np.max(q_y[next_state])- q_y[state, optact])\n",
    "                        \n",
    "                        ep_reward += reward\n",
    "                        count += 1\n",
    "                        q_intra[state,action] += ALPHA * (reward + GAMMA * np.max(q_intra[next_state, :])- q_intra[state, action])\n",
    "                        state = next_state\n",
    "                \n",
    "            running = 0.35 * ep_reward + (1 - 0.35) * running\n",
    "            self.avg_reward.append(running)\n",
    "            self.q=q_intra\n",
    "            self.total_reward.append(ep_reward)\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {ep_reward}\")\n",
    "        return self.avg_reward, self.total_reward\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.plot(self.avg_reward,label=\"Moving Average\")\n",
    "        plt.grid()\n",
    "        plt.plot(self.total_reward,label=\"Total Rewards\",alpha=1, color='orange')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Averaged Episodic Returns')\n",
    "        plt.title(\"Averaged Episodic Returns\")\n",
    "        plt.legend()\n",
    "        plt.savefig('SMDP.png')\n",
    "        plt.show()\n",
    "\n",
    "    def render_run(self):\n",
    "        special_env = gym.make('Taxi-v3',render_mode='human')\n",
    "        done = False\n",
    "        eps = 0.01\n",
    "        state, _ = special_env.reset()\n",
    "        while not done:\n",
    "            action = choose_action(state,self.q,eps)\n",
    "            next_state, reward, done, _, _ = special_env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = INTRAOPT()\n",
    "agent2.Qlearn()\n",
    "agent2.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
